{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MP_NDIAYE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MP_NDIAYE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MP_NDIAYE\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger les ressources nécessaires de NLTK (si vous ne l'avez pas déjà fait)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\MP_NDIAYE\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier texte et prétraiter les données\n",
    "with open('ww2.txt','r', encoding='utf-8') as f:\n",
    "    data = f.read().replace('\\n', ' ')\n",
    "    f.close()\n",
    "# Tokeniser le texte en phrases\n",
    "sentences = sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour. Bonjour, monsieur vous désirez ? J’ai mal à l’estomac depuis ce matin. Est-ce que vous auriez quelque chose pour calmer les maux d’estomac ? Oui, j’ai le spasmoapothel. C’est très efficace. Vous en prenez un après les repas. Très bien,le prix. 15 euros monsieur. Voilà, monsieur. Merci monsieur. Au revoir. je voudrais quelque chose pour ma femme. Elle a pris froid et elle est enrhumée, elle tousse beaucoup, elle éternue et elle a mal à la gorge. Est-ce qu’elle a vu un médecin ? Non, elle pense que c’est seulement un rhume. Elle n’a pas de fièvre ? Non. Bien, on va lui donner un sirop pour la toux, des pastilles pour la gorge, de l’aspirine. Et si elle ne se sent pas mieux dans les quatre ou cinq jours, je lui conseille quand même d’aller voir un médecin. Merci monsieur. Au revoir, bonne journée.\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour.', 'Bonjour, monsieur vous désirez ?', 'J’ai mal à l’estomac depuis ce matin.', 'Est-ce que vous auriez quelque chose pour calmer les maux d’estomac ?', 'Oui, j’ai le spasmoapothel.', 'C’est très efficace.', 'Vous en prenez un après les repas.', 'Très bien,le prix.', '15 euros monsieur.', 'Voilà, monsieur.', 'Merci monsieur.', 'Au revoir.', 'je voudrais quelque chose pour ma femme.', 'Elle a pris froid et elle est enrhumée, elle tousse beaucoup, elle éternue et elle a mal à la gorge.', 'Est-ce qu’elle a vu un médecin ?', 'Non, elle pense que c’est seulement un rhume.', 'Elle n’a pas de fièvre ?', 'Non.', 'Bien, on va lui donner un sirop pour la toux, des pastilles pour la gorge, de l’aspirine.', 'Et si elle ne se sent pas mieux dans les quatre ou cinq jours, je lui conseille quand même d’aller voir un médecin.', 'Merci monsieur.', 'Au revoir, bonne journée.']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    \"\"\"Définir une fonction pour prétraiter chaque phrase\"\"\"\n",
    "    # Tokeniser la phrase en mots\n",
    "    words = word_tokenize(sentence)\n",
    "    # Enlever les stopwords et la ponctuation\n",
    "    words = [word.lower() for word in words if word.lower() not in stopwords.words('english') and word not in string.punctuation]\n",
    "    # Lemmatiser les mots\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bonjour'], ['bonjour', 'monsieur', 'vous', 'désirez'], ['j', '’', 'ai', 'mal', 'à', 'l', '’', 'estomac', 'depuis', 'ce', 'matin'], ['est-ce', 'que', 'vous', 'auriez', 'quelque', 'chose', 'pour', 'calmer', 'le', 'maux', '’', 'estomac'], ['oui', 'j', '’', 'ai', 'le', 'spasmoapothel']]\n"
     ]
    }
   ],
   "source": [
    "# Appliquer la fonction de prétraitement sur chaque phrase\n",
    "preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "# Affichage d'une phrase prétraitée pour vérification\n",
    "print(preprocessed_sentences[:5])  # Affiche les 5 premières phrases prétraitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_sentence(query):\n",
    "    \"\"\"Définir une fonction pour trouver la phrase la plus pertinente en fonction d'une requête\"\"\"\n",
    "    # Prétraiter la requête\n",
    "    query = preprocess(query)\n",
    "    # Calculer la similarité entre la requête et chaque phrase du texte\n",
    "    max_similarity = 0\n",
    "    most_relevant_sentence = \"\"\n",
    "    for sentence in corpus:\n",
    "        similarity = len(set(query).intersection(sentence)) / float(len(set(query).union(sentence)))\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_relevant_sentence = \" \".join(sentence)\n",
    "    return most_relevant_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_relevant_sentence(query, corpus):\n",
    "    \"\"\"\n",
    "    Trouver la phrase la plus pertinente en fonction d'une requête.\n",
    "    \n",
    "    Args:\n",
    "        query (str): La requête en texte.\n",
    "        corpus (list): Liste de phrases prétraitées (chaque phrase étant une liste de mots).\n",
    "    \n",
    "    Returns:\n",
    "        str: La phrase la plus pertinente en texte.\n",
    "    \"\"\"\n",
    "    # Prétraiter la requête\n",
    "    query_processed = preprocess(query)\n",
    "    \n",
    "    # Initialiser les variables de similarité maximale\n",
    "    max_similarity = 0\n",
    "    most_relevant_sentence = \"\"\n",
    "    \n",
    "    # Parcourir chaque phrase dans le corpus\n",
    "    for sentence in corpus:\n",
    "        # Calculer la similarité Jaccard entre la requête et la phrase actuelle\n",
    "        similarity = len(set(query_processed).intersection(set(sentence))) / float(len(set(query_processed).union(set(sentence))))\n",
    "        \n",
    "        # Mettre à jour si une similarité plus élevée est trouvée\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_relevant_sentence = \" \".join(sentence)\n",
    "    return most_relevant_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase la plus pertinente : j ’ ai mal à l ’ estomac depuis ce matin\n"
     ]
    }
   ],
   "source": [
    "query = \"J’ai mal à l’estomac depuis ce matin.\"\n",
    "most_relevant = get_most_relevant_sentence(query, preprocessed_sentences)\n",
    "print(\"Phrase la plus pertinente :\", most_relevant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explications des corrections :\n",
    "# 1.Chargement et prétraitement du fichier texte :\n",
    "\n",
    "La fonction load_and_preprocess_text est ajoutée pour charger le fichier texte et le prétraiter avant de l'utiliser dans le chatbot. Cela permet de séparer le code de prétraitement et de le réutiliser facilement.\n",
    "# 2.Prétraitement de la requête utilisateur :\n",
    "\n",
    "La requête utilisateur est prétraitée de la même manière que les phrases du corpus afin de garantir une correspondance cohérente.\n",
    "# 3.Bouton Streamlit :\n",
    "\n",
    "Le bouton \"Soumettre\" vérifie que l'utilisateur a bien entré une question avant d'appeler la fonction get_most_relevant_sentence. Sinon, un message d'erreur est affiché.\n",
    "# 4.Chemin du fichier :\n",
    "\n",
    "Assurez-vous que le chemin du fichier ww2.txt est correct. Vous pouvez le modifier selon vos besoins.\n",
    "# 5.Gestion des ressources NLTK :\n",
    "\n",
    "J'ai inclus les téléchargements des ressources NLTK en haut du script pour éviter les erreurs liées aux ressources manquantes.\n",
    "Cette application fonctionnera dans Streamlit et permettra aux utilisateurs de poser des questions basées sur un fichier texte prétraité. Pour exécuter l'application, utilisez la commande suivante dans le terminal :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_text(file_path):\n",
    "    \"\"\" Charger le fichier texte et le prétraiter \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.read().replace('\\n', ' ')\n",
    "    sentences = sent_tokenize(data)\n",
    "    preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n",
    "    return preprocessed_sentences\n",
    "\n",
    "def preprocess(sentence):\n",
    "    \"\"\" Fonction de prétraitement pour chaque phrase \"\"\"\n",
    "    words = word_tokenize(sentence)\n",
    "    words = [word.lower() for word in words if word.lower() not in stopwords.words('english') and word not in string.punctuation]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words\n",
    "\n",
    "def get_most_relevant_sentence(query, corpus):\n",
    "    \"\"\" Fonction pour trouver la phrase la plus pertinente \"\"\"\n",
    "    query_processed = preprocess(query)\n",
    "    max_similarity = 0\n",
    "    most_relevant_sentence = \"\"\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        similarity = len(set(query_processed).intersection(set(sentence))) / float(len(set(query_processed).union(set(sentence))))\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            most_relevant_sentence = \" \".join(sentence)\n",
    "    \n",
    "    return most_relevant_sentence\n",
    "\n",
    "def main():\n",
    "    \"\"\" Créer l'application Streamlit \"\"\"\n",
    "    st.title(\"Chatbot\")\n",
    "    st.write(\"Bonjour! Je suis un chatbot. Demandez-moi quoi que ce soit sur le sujet dans le fichier texte.\")\n",
    "    \n",
    "    # Charger et prétraiter le fichier texte\n",
    "    file_path = 'ww2.txt'  # Remplacez par le chemin correct de votre fichier\n",
    "    preprocessed_sentences = load_and_preprocess_text(file_path)\n",
    "    \n",
    "    # Obtenir la question de l'utilisateur\n",
    "    question = st.text_input(\"Vous:\")\n",
    "    \n",
    "    # Créer un bouton pour soumettre la question\n",
    "    if st.button(\"Soumettre\"):\n",
    "        if question:\n",
    "            response = get_most_relevant_sentence(question, preprocessed_sentences)\n",
    "            st.write(\"Chatbot: \" + response)\n",
    "        else:\n",
    "            st.write(\"Veuillez entrer une question.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pour créer un chatbot basique en utilisant la bibliothèque NLTK (Natural Language Toolkit) en Python, voici les étapes essentielles. NLTK est une bibliothèque puissante pour le traitement du langage naturel et peut être utilisée pour construire des chatbots avec des fonctionnalités limitées (répondre à des questions simples ou matcher des phrases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explication :\n",
    "* Paires : Ce sont les règles simples définissant les questions possibles et les réponses associées.\n",
    "* Reflections : Cela aide à convertir des pronoms tels que \"je\" en \"tu\" pour rendre la conversation plus naturelle.\n",
    "* Chat Class : Chat est l'outil principal de NLTK pour gérer les dialogues simples avec des paires de correspondance.\n",
    "## Limites :\n",
    "Ce type de chatbot est assez limité car il n'apprend pas et ne comprend pas réellement le contexte des conversations.\n",
    "Pour un chatbot plus complexe, vous pouvez explorer des algorithmes d'apprentissage profond ou utiliser des outils comme spaCy, Transformers, ou des frameworks comme Rasa ou Dialogflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.chat.util import Chat, reflections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    [\n",
    "        r\"bonjour|salut|coucou\",\n",
    "        [\"Bonjour, comment puis-je vous aider ?\", \"Salut, ça va ?\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"comment (vas|allez)-tu ?\",\n",
    "        [\"Je vais bien, merci ! Et vous ?\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"quel est ton nom ?\",\n",
    "        [\"Je suis un chatbot créé avec NLTK. Et vous, quel est votre nom ?\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"adieu|au revoir\",\n",
    "        [\"Au revoir ! Bonne journée !\", \"À bientôt !\"]\n",
    "    ],\n",
    "    [\n",
    "        r\"je m'appelle (.*)\",\n",
    "        [\"Ravi de vous rencontrer %1\"]\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = Chat(pairs, reflections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_chat():\n",
    "    print(\"Bonjour, je suis un chatbot. Tapez quelque chose pour commencer la conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"> \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            print(\"Au revoir !\")\n",
    "            break\n",
    "        response = chatbot.respond(user_input)\n",
    "        print(response)\n",
    "\n",
    "start_chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
